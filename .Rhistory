iris$cluster <- as.factor(km.out$cluster)
ggplot(data = iris) +
geom_point(mapping = aes(Petal.Length, Petal.Width, color = iris$cluster))
km.out <- kmeans(iris[, 3:4], centers = 3, nstart = 20)
table(km.out$cluster, iris$Species)
iris$cluster <- as.factor(km.out$cluster)
ggplot(data = iris) +
geom_point(mapping = aes(Petal.Length, Petal.Width, color = iris$cluster))
centers  <- apply(data, 2, tapply, clusters, mean)
clusters <- sample(1:3, nrow(data), replace = TRUE)
centers  <- apply(data, 2, tapply, clusters, mean)
data <- iris[, 3:4]
clusters <- sample(1:3, nrow(data), replace = TRUE)
centers  <- apply(data, 2, tapply, clusters, mean)
centers
dist <- function(p1, p2) {
return(sum((p1 - p2)^2))
}
point.assign <- function(point, centers) {
# Input: one point
# Output: which cluster center is closest
return(which.min(c(dist(point, centers[1, ]),
dist(point, centers[2, ]),
dist(point, centers[3, ]))))
}
new.clusters <- function(points, centers) {
return(apply(points, 1, point.assign, centers))
}
new.clus <- new.clusters(points = data, centers = centers)
head(data)
apply
while(any(new.clus != clusters)) {
clusters <- new.clus
centers  <- apply(data, 2, tapply, clusters, mean)
# calculate centers for each row and each cluster
# clusters and mean are the additional parameters to be passed to tapply
# tapply(data, clusters, mean)
new.clus <- new.clusters(points = data, centers = centers)
}
table(new.clus, iris$Species)
?colmeans
??colmeans
a<-as.logical(T,F,F,T)
a
a<-as.logical(c)T,F,F,T)
a<-as.logical(c(T,F,F,T))
a
b<-as.logical(c(T,T,T))
a & b
b<-as.logical(c(T,T,T,F))
a& b
?sample
qqplot(y, rt(300, df = 5))
y <- rt(200, df = 5)
qqnorm(y); qqline(y, col = 2)
my.array           <- array(1:27, c(3,3,3))
rownames(my.array) <- c("R1", "R2", "R3")
colnames(my.array) <- c("C1", "C2", "C3")
dimnames(my.array)[[3]] <- c("Bart", "Lisa", "Maggie")
my.array
my.array           <- array(1:81, c(3,3,3,3))
my.array
qqplot(y, rt(300, df = 5))
my.array[,2,2,]
my.list <- list(nums = rnorm(1000), lets = letters, pops = state.x77[ ,"Population"])
head(my.list[[1]], 5)
head(my.list[[2]], 5)
head(my.list[[3]], 5)
llply(my.list, range) # Get back a list
laply(my.list, summary) # wouldn't work
ldply(my.list, summary)
llply(my.list, summary) # Works just fine
library(plyr)
llply(my.list, range) # Get back a list
laply(my.list, summary) # wouldn't work
ldply(my.list, summary)
llply(my.list, summary) # Works just fine
laply(my.list,range)
ldply(my.list,range)
a_ply(my.array, 2:3, plot, ylim = range(my.array), pch = 19, col = 6)
a_ply(my.array, 1:3, plot, ylim = range(my.array), pch = 19, col = 6)
a_ply
?a_ply
# number of desired clusters,
km.out <- kmeans(x, centers = 2, nstart = 20)
kmeans
x <- matrix(rnorm(50*2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
head(x, 5)
# number of desired clusters,
km.out <- kmeans(x, centers = 2, nstart = 20)
km.out
km.out$clusters
km.out$cluster
data <- iris[, 3:4]
# For each of the K clusters, compute the cluster centriod. The kth
# centroid is the vector of the p feature means (covariate means) for the
# observations in the kth cluster.
# code the algo ourselfes
# randomly assign clusters
clusters <- sample(1:3, nrow(data), replace = TRUE)
# cluster means
centers  <- apply(data, 2, tapply, clusters, mean)
centers
tapply(iris[,3],clusters,mean)
a<-tapply(iris[,3],clusters,mean)
typeof(a)
a[1]
typeof(centeres)
typeof(centers)
center[1,1]
centers[1,1]
a
my.array
my.array[1,2,3,]
my.array[1,2,3]
my.array[1,2,3,1]
g <- airquality$Month
l <- split(airquality, g)
l
sum(l)
nrow(l)
l <- lapply(l, transform, Oz.Z = scale(Ozone))
l <- lapply(l, length)
l
g
l
g <- airquality$Month
airquality
dim(airquality)
colnames(airquality)
split(airquality,a,Wind)
split(airquality,g,Wind)
split(airquality,g & Wind)
split(airquality,c(g,airquality$Ozone))
split(airquality,list(g,airquality$Ozone))
aM-split(airquality,list(g,airquality$Ozone))
a<-split(airquality,list(g,airquality$Ozone))
lapply(a,length)
a<-split(airquality,list(g,airquality$Ozone))
a
head(airquality)
a<-split(airquality,list(g,airquality$Day))
a
a<-split(airquality,list(g,I(airquality$Day>15)))
a
lapply(a,length)
a
lapply(a,nrow)
library(ply)
library(plyr)
dlply(airquality,.(Month,I(Day>10)),nrow)
airquality["Day"=5]
airquality["Day"=5,]
airquality[Day=5,]
airquality[airquality$Day=5,]
airquality[airquality$Day==5,]
airquality[Day==5,]
which(airquality$Day==15)
density(1:10)
plot(density(1:10))
plot(density(2:100))
data<-matrix(seq(1:100),nrow=10)
dgamma(data)
dgamma(data,shape=1,scale=2)
d<-dgamma(data,shape=1,scale=2)
dim(d)
sm(d)
sum(d)
d<-dgamma(data,shape=1,scale=2,log=T)
sum(d)
factory.n <- list(c("labor","steel"), c("car","truck"))
factory   <- matrix(c(40, 1, 60, 3), nrow = 2, dimnames = factory.n)
available        <- c(1600, 70)
names(available) <- rownames(factory)
prices           <- c(car = 13, truck = 27)
factory
factory %*% c(5,5)
c(5,5) %*%  factory
sapply(1:2,rep,1:2)
sapply(1:2,rep)
data
mapply(data,mean)
mapply(mean,data)
a<-rep(c(a,b),c(4,4))
a<-rep(c("a","b"),c(4,4))
a
aggregate(1:8,a,mean)
aggregate(1:8,list(a),mean)
?tapply
tapply(1:8,a,range)
tapply(1:8,a,range)[2]
tapply(1:8,a,range)[1]
tapply(1:8,a,range)[1,1]
tapply(1:8,a,range)[1]
typeof(tapply(1:8,a,range)[1]
)
aggregate(tapply(1:8,list(a),range)
)
aggregate(1:8,list(a),range)
km.out <- kmeans(1:100, centers = 2, nstart = 20)
km.out
summary(km.out)
km.out$tot.withinss
km.out <- kmeans(iris[, 3:4], centers = 3, nstart = 20)
table(km.out$cluster, iris$Species)
install.packages("tidyverse")
library(tidyverse)
install.packages("readr")
install.packages("DT")
library(DT)
datatable(iris, options = list(pageLength = 5))
datatable(iris, options = list(pageLength = 5))
tibble(x=letters)
library(tibble)
tibble(x=letters)
tibble(x = 1:3, y = list(1:5, 1:10, 1:20))
a<-tibble(x = 1:3, y = list(1:5, 1:10, 1:20))
a
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english")) # remove words that is only important to the grammar structure
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
tdm.all<-TermDocumentMatrix(ff.all)
isntall.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
---
title: "R Notebook"
output: html_notebook
author: Yue Jin
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=3,warning=FALSE, message=FALSE,echo=FALSE)
setwd("/Users/yuejin/Dropbox/Courseworks/ADS/project1/Spr2017-Proj1-yuejin123")
```
## Step 1 - Data Preparation
- Set environment
```{r}
# Install and load libraries
packages.used=c("tm", "wordcloud", "RColorBrewer",
"dplyr", "tidytext")
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
# load functions
source("../lib/speechFuncs.R")
```
- Import data
```{r}
## URL Approach
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
inaug=f.speechlinks(main.page)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"
# Incorporate economic data
gdp=read.csv("../data/download.csv",colClasses = c("double",NULL,"double"))
# scrap the main text from the website
inaug.list$fulltext=NA
for(i in seq(nrow(inaug.list))) {
text <- read_html(inaug.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
inaug.list$fulltext[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
inaug.list$type[i],
inaug.list$File[i], "-",
inaug.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
folder.path="../data/InauguralSpeeches/"
ff.all<-Corpus(DirSource(folder.path))
```
- Remove extra white space, convert all letters to the lower case, remove stop words, removed empty words due to formatting errors, and remove punctuation.
```{r, include= FALSE}
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
```
# Topic Modeling
- LDA
```{r}
dtm <- DocumentTermMatrix(ff.all)
rownames(dtm) <- paste0(inaug.list$type, inaug.list$File,"_",inaug.list$Term)
rowTotals <- apply(dtm , 1, sum)
dtm  <- dtm[rowTotals> 0, ]
burnin <- 4000
iter <- 100
thin <- 40
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
# Number of topics
k <- 5
# Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
```
```{r}
# write out results
# docs to topics
# Extract the most likely terms for each topic or the most likely topics for each document.
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))
# each row represents a document, and rowSum equals 1
# logarithmized parameters of the word distribution for each topic.
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
# select 7 keywords for each topic
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```
# Generate List of Sentences
sentence.list=NULL
for(i in 1:nrow(speech.list)){
sentences=sent_detect(speech.list$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
# devide the text into sentences; create sentiment scores
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech.list[i,-ncol(speech.list)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
# to select sentences
sentence.list.sel=sentence.list%>%filter(type=="inaug", File%in%sel.comparison, Term==1)
sentence.list.sel$File=factor(sentence.list.sel$File)
sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File,
sentence.list.sel$word.count,
mean,
order=T)
```
# Text Mining
![image](http://www.staffordschools.net/cms/lib011/VA01818723/Centricity/Domain/3574/character_icon.png)
```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
#Stem document
docs <- tm_map(docs,stemDocument)
```
# Topic Modeling
```{r}
```
# Write Output
```{r}
write.csv(ldaOut.topics,file=paste("../out/LDAGibbs",k,"DocsToTopics.csv"))
```
# Reference
+ [Text mining with `tidytext`](http://tidytextmining.com/).
+ [Basic Text Mining in R](https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html)
getwd()
knitr::opts_chunk$set(fig.width=4, fig.height=3,warning=FALSE, message=FALSE,echo=FALSE)
setwd("/Users/yuejin/Dropbox/Courseworks/ADS/project1/Spr2017-Proj1-yuejin123")
packages.used=c("tm", "wordcloud", "RColorBrewer",
"dplyr", "tidytext")
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
source("../lib/speechFuncs.R")
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"
gdp=read.csv("../data/download.csv",colClasses = c("double",NULL,"double"))
inaug.list$fulltext=NA
inaug$Date
inaug$date
names(inaug)
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list$type="inaug"
gdp=read.csv("../data/download.csv",colClasses = c("double",NULL,"double"))
names(inaug.list)
str(inaug.list)
str(inaug)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page)
as.Date(inaug[,1], format="%B %e, %Y")
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"
str(inaug.list)
as.Date(inaug.list$Date, format="%B %e, %Y")
tail(inaug.list$Date)
tail(inaug)
as.Date(inaug.list$Date, format="%B %e, %Y")
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page)
as.Date(inaug.list$Date, format="%B %e, %Y")
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page)
as.Date(inaug.list$Date, format="%B %e, %Y")
as.Date(inaug$Date, format="%B %e, %Y")
as.Date(inaug[,1], format="%B %e, %Y")
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
inaug=f.speechlinks(main.page)
as.Date(inaug[,1], format="%B %e, %Y")
length(inaug)
dim(inaug)
gdp
sdf
gdp=read.csv("../data/download.csv",colClasses = c("double",NULL,"double"))
getwd()
gdp=read.csv("../data/download.csv",colClasses = c("double",NULL,"double"))
names(inaug.list)
for(i in seq(nrow(inaug.list))) {
text <- read_html(inaug.list$urls[i]) %>% # load the page
html_nodes(".displaytext") %>% # isloate the text
html_text() # get the text
inaug.list$fulltext[i]=text
# Create the file name
filename <- paste0("../data/InauguralSpeeches/",
inaug.list$type[i],
inaug.list$File[i], "-",
inaug.list$Term[i], ".txt")
sink(file = filename) %>% # open file to write
cat(text)  # write the file
sink() # close the file
}
rm(list=ls())
