---
title: "R Notebook"
author: "Yue Jin"
output:
  pdf_document: default
  html_notebook: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=4,fig.height=3,warning=FALSE,message=FALSE,echo=FALSE)
setwd("/Users/yuejin/Dropbox/Courseworks/ADS/project1/Spr2017-Proj1-yuejin123")
```


## Step 1 - Data Preparation
- Set environment
```{r,message=FALSE,warning=FALSE}

# Install and load libraries
packages.used=c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tidytext")
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}


library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

# load functions
source("../lib/speechFuncs.R")
source("../lib/plotstacked.R")
```

- Import data
```{r}
## URL Approach
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
inaug=f.speechlinks(main.page)
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"


# Incorporate economic data
gdp=read.csv("../data/download.csv",skip=1,header=TRUE)
gdp=gdp[,-2]
names(gdp) <- c("year","growth")
gdp$year=as.Date(paste(gdp$year,"-12-31",sep=""))


inaug.list$Date=as.Date(inaug[,1], format="%B %e, %Y")

inaug.list=filter(inaug.list,Date>gdp$year[1])
inaug.list$growth=NA
for (i in seq(nrow(inaug.list)-1)){
  years<-filter(gdp,year>inaug.list$Date[i] & year<=inaug.list$Date[i+1])
  inaug.list$growth[i]=mean(years$growth)
}



# scrap the main text from the website
inaug.list$fulltext=NA
for(i in seq(nrow(inaug.list))) {
  text <- read_html(inaug.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  inaug.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/InauguralSpeeches/", 
                     inaug.list$type[i], 
                     inaug.list$File[i], "-", 
                     inaug.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}


```

- Remove extra white space, convert all letters to the lower case, remove stop words, removed empty words due to formatting errors, and remove punctuation. 
```{r, include= FALSE}
folder.path="../data/InauguralSpeeches/"
ff.all<-Corpus(DirSource(folder.path))

ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
ff.all<-tm_map(ff.all, stemDocument)
```

# Topic Modeling

- LDA
```{r}
dtm <- DocumentTermMatrix(ff.all)
rownames(dtm) <- paste0(inaug.list$type, inaug.list$File,"_",inaug.list$Term)
rowTotals <- apply(dtm , 1, sum) 
dtm  <- dtm[rowTotals> 0, ]

burnin <- 4000
iter <- 1000
thin <- 500
seed <-list(2003,2,68,100001,765)
nstart <- 5
best <- TRUE

# Number of topics
k <- 15

# Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter=iter,
                                                 thin=thin))



```

Extract the most likely terms for each topic or the most likely topics for each document.
```{r}
# write out results
# docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))
ldaOut.terms <- as.matrix(terms(ldaOut,15))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))
# each row represents a document, and rowSum equals 1

# logarithmized parameters of the word distribution for each topic.
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL

# select 10 keywords for each topic

for(i in 1:k){
  topics.terms=cbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:10]])
}

write.csv(topics.terms,file=paste("../output/LDAGibbs",k,"TopicsKeywords.csv"))

topics.terms
ldaOut.terms
```

- Summarize Each Topic with One Word
```{r}
topics.hash=c("Economy", "Defense", "America", "Community", "Leadership", "Democracy", "Unity", "Government", "Reform", "Temporal", "WorkingFamilies", "Freedom", "Equality", "Misc", "Legislation")

inaug.list$ldatopic=as.vector(ldaOut.topics)
# assign a word to a corpus
inaug.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
# add the prob of topic of each document to the corpus list df
inaug.list.df=cbind(inaug.list, topicProbabilities)

```

- Clustering of topics

```{r, fig.width=3, fig.height=4}
par(mar=c(1,1,1,1)) 

topic.summary=tbl_df(inaug.list.df)%>%
              group_by(File)%>%
              select(File, Economy:Legislation)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]

topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")

inaug.list.df$growth[nrow(inaug.list.df)]=0
inaug.list.df$growth2=NULL
inaug.list.df$growth2<-cut(inaug.list.df$growth,breaks=5)

topic.summary2=tbl_df(inaug.list.df)%>%
              group_by(growth2)%>%
              select(growth2, Economy:Legislation)%>%
              summarise_each(funs(mean))
topic.summary2=as.data.frame(topic.summary2)
rownames(topic.summary2)=topic.summary2[,1]

topic.plot=c(1:3,3:7)


heatmap.2(as.matrix(topic.summary2[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
```

- How topics shift as economic condition changes
```{r, fig.width=3, fig.height=4}


topic.plot=c(1, 13, 14, 15, 8, 9, 12)

speech.df=tbl_df(inaug.list.df)%>%select(growth, ... = Economy:Legislation)


# speech.df=tbl_df(inaug.list.df)%>%group_by(growth2)%>%select(growth2, ... = Economy:Legislation)
# speech.df[,1]<-as.numeric(factor(speech.df[,1],labels=c("1","2","3","4")))

speech.df=data.frame(speech.df)
speech.df=as.matrix(speech.df[order(speech.df$growth),])

speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/30, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
             xlab="GDP growth", ylab="Topic share", main="Topic Shifts")
```

## How much topics differ given different economic conditions
```{r}
presid.summary=tbl_df(inaug.list.df)%>%
  select(growth, Economy:Legislation)%>%
  group_by(growth)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
              3)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)

```



# Reference

+ [Text mining with `tidytext`](http://tidytextmining.com/).
+ [Basic Text Mining in R](https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html)
