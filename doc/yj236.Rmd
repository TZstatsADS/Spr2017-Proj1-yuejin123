---
title: "Economic Conditions and the Inaugural Speeches"
author: "Yue Jin"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

![image](https://i1.wp.com/headsup.boyslife.org/files/2017/01/20130121_inauguration20_53.jpg)

# Data Preparation
- Set environment: install and load libraries; chunk options


```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message=FALSE,echo=FALSE)
setwd("/Users/yuejin/Dropbox/Courseworks/ADS/project1/Spr2017-Proj1-yuejin123")
```

```{r,message=FALSE,warning=FALSE}

# Install and load libraries
packages.used=c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tidytext")
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}


library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

# load functions
source("../lib/speechFuncs.R")
source("../lib/plotstacked.R")
```

- Import data
  + Only data/speeches collected after 1929 are included due to the availability of economic data
  + Scrap inaugural speeches from the [website](http://www.presidency.ucsb.edu/inaugurals.php)
  + Import GDP growth data from the local files
```{r}
## URL Approach
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
inaug=f.speechlinks(main.page)
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
inaug.list=cbind(inaug.list,inaug)
inaug.list$type="inaug"


# Incorporate economic data
gdp=read.csv("../data/download.csv",skip=1,header=TRUE)
gdp=gdp[,-2]
names(gdp) <- c("year","growth")
gdp$year=as.Date(paste(gdp$year,"-12-31",sep=""))


inaug.list$Date=as.Date(inaug[,1], format="%B %e, %Y")

inaug.list=filter(inaug.list,Date>gdp$year[1])
inaug.list$growth=NA
for (i in seq(nrow(inaug.list)-1)){
  years<-filter(gdp,year>inaug.list$Date[i] & year<=inaug.list$Date[i+1])
  inaug.list$growth[i]=mean(years$growth)
}



# scrap the main text from the website
inaug.list$fulltext=NA
for(i in seq(nrow(inaug.list))) {
  text <- read_html(inaug.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  inaug.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/InauguralSpeeches/", 
                     inaug.list$type[i], 
                     inaug.list$File[i], "-", 
                     inaug.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}


```

- Inital data processing 
  + Remove extra white space, convert all letters to the lower case, remove stop words, removed empty words due to formatting errors, and remove punctuation
```{r, include= FALSE}
folder.path="../data/InauguralSpeeches/"
ff.all<-Corpus(DirSource(folder.path))

ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
ff.all<-tm_map(ff.all, stemDocument)
```

# Topic Modeling

- Run LDA on our speech data
 + Set number of topics as 15
```{r}
dtm <- DocumentTermMatrix(ff.all)
rownames(dtm) <- paste0(inaug.list$type, inaug.list$File,"_",inaug.list$Term)
rowTotals <- apply(dtm , 1, sum) 
dtm  <- dtm[rowTotals> 0, ]

burnin <- 4000
iter <- 1000
thin <- 500
seed <-list(2003,2,68,100001,765)
nstart <- 5
best <- TRUE

# Number of topics
k <- 15

# Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter=iter,
                                                 thin=thin))



```

+ Extract the 15 most likely terms and the 10 most salient terms for each topic (only displaying the first 5)
```{r}
# write out results
# docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))
ldaOut.terms <- as.matrix(terms(ldaOut,15))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))
# each row represents a document, and rowSum equals 1

# logarithmized parameters of the word distribution for each topic.
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL

# select 10 keywords for each topic

for(i in 1:k){
  topics.terms=cbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:10]])
}

write.csv(topics.terms,file=paste("../output/LDAGibbs",k,"TopicsKeywords.csv"))
```

  + Summarize Each Topic with One Word
  topics.hash=c("Tax", "Defense", "America", "Economy", "America", "Prosperity", "Leadership", "Reform", "Liberty", "International", "Nation", "Freedom", "Democracy", "Misc", "Power")

```{r}
topics.hash=c("Tax", "Defense", "Crisis", "Economy", "America", "Prosperity", "Leadership", "Reform", "Liberty", "International", "Nation", "Freedom", "Democracy", "Misc", "Power")

inaug.list$ldatopic=as.vector(ldaOut.topics)
# assign a word to a corpus
inaug.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
# add the prob of topic of each document to the corpus list df
inaug.list.df=cbind(inaug.list, topicProbabilities)

```

## How topic shifts given different economic conditions
 + Inspect how economic condition changes over time
 
```{r}
par(mar=c(1,1,2,0))
plot(inaug.list.df$Date,inaug.list.df$growth,ylab="GDP Growth %",xlab="Year",main="Economic condition Over Time",type="l",col=2)
```

## What topics are more heated during different periods of time
 - During more recent times (GDP growth rate 0-3%), alll topics are mentioned in the inaugural speeches. During more prosperous years, presidents talked more about *Reform*, *Prosperity*,*Derense*. During distressed time, presidents focused attention on *America* and *Economy*.

```{r}
par(mar=c(1,1,1,1)) 


inaug.list.df$growth[nrow(inaug.list.df)]=0
inaug.list.df$growth2=NULL
inaug.list.df$growth2<-cut(inaug.list.df$growth,breaks=5)

topic.summary2=tbl_df(inaug.list.df)%>%
              group_by(growth2)%>%
              select(growth2, Tax:Power)%>%
              summarise_each(funs(mean))
topic.summary2=as.data.frame(topic.summary2)
rownames(topic.summary2)=topic.summary2[,1]

topic.plot=c(2:6,7,8,13)


heatmap.2(as.matrix(topic.summary2[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
```

## How topics shift as economic condition changes
```{r}

par(mar=c(1,1,2,0))
topic.plot=c(2:6,7,8,13)

speech.df=tbl_df(inaug.list.df)%>%select(growth, ... = Tax:Power)


# speech.df=tbl_df(inaug.list.df)%>%group_by(growth2)%>%select(growth2, ... = Tax:Power)
# speech.df[,1]<-as.numeric(factor(speech.df[,1],labels=c("1","2","3","4")))

speech.df=data.frame(speech.df)
speech.df=as.matrix(speech.df[order(speech.df$growth),])

speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/30, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
             xlab="GDP growth", ylab="Topic share", main="Topic Shifts")
```

## How much topics differ given different economic conditions
```{r}
par(mar=c(1,1,2,0))
presid.summary=tbl_df(inaug.list.df)%>%
  select(growth, Tax:Power)%>%
  group_by(growth)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
              3)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)

```


# Who is the president in the year with highest/lowest GDP growth rate?
```{r}
inaug.list.df[which.max(inaug.list.df$growth),]$President
inaug.list.df[which.max(inaug.list.df$growth),]$fulltext
inaug.list.df[which.min(inaug.list.df$growth),]$President
inaug.list.df[which.min(inaug.list.df$growth),]$fulltext
```


# Reference

+ [Text mining with `tidytext`](http://tidytextmining.com/).
+ [Basic Text Mining in R](https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html)
